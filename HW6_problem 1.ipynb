{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c7c1e7-0c82-4801-8c3e-244020e03e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "===== TRAINING ViT_ps4_ed256_d4_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed256_d4_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed256_d4_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed256_d4_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed256_d8_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed256_d8_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed256_d8_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed256_d8_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d4_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d4_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d4_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d4_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d8_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d8_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d8_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps4_ed512_d8_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d4_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d4_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d4_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d4_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d8_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d8_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d8_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed256_d8_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d4_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d4_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d4_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d4_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d8_h2_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d8_h2_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d8_h4_mr2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ViT_ps8_ed512_d8_h4_mr4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahosain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ahosain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAINING ResNet-18 CIFAR100 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL RESULTS ===\n",
      "Model                                    | Params       | FLOPs        | Time/Ep(s) | Test Acc\n",
      "------------------------------------------------------------------------------------------\n",
      "ViT_ps4_ed256_d4_h2_mr2                  |    2,164,068 |    1,884,772 |      0.019 |   35.72%\n",
      "ViT_ps4_ed256_d4_h2_mr4                  |    3,214,692 |    2,935,396 |      0.020 |   39.71%\n",
      "ViT_ps4_ed256_d4_h4_mr2                  |    2,164,068 |    1,884,772 |      0.019 |   10.86%\n",
      "ViT_ps4_ed256_d4_h4_mr4                  |    3,214,692 |    2,935,396 |      0.020 |   35.24%\n",
      "ViT_ps4_ed256_d8_h2_mr2                  |    4,272,484 |    2,940,516 |      0.025 |    2.69%\n",
      "ViT_ps4_ed256_d8_h2_mr4                  |    6,373,732 |    5,041,764 |      0.026 |   10.11%\n",
      "ViT_ps4_ed256_d8_h4_mr2                  |    4,272,484 |    2,940,516 |      0.025 |   13.56%\n",
      "ViT_ps4_ed256_d8_h4_mr4                  |    6,373,732 |    5,041,764 |      0.026 |   18.86%\n",
      "ViT_ps4_ed512_d4_h2_mr2                  |    8,522,340 |    5,866,596 |      0.022 |    4.98%\n",
      "ViT_ps4_ed512_d4_h2_mr4                  |   12,720,740 |   10,064,996 |      0.026 |    3.93%\n",
      "ViT_ps4_ed512_d4_h4_mr2                  |    8,522,340 |    5,866,596 |      0.022 |    4.84%\n",
      "ViT_ps4_ed512_d4_h4_mr4                  |   12,720,740 |   10,064,996 |      0.025 |    4.63%\n",
      "ViT_ps4_ed512_d8_h2_mr2                  |   16,933,476 |   10,075,236 |      0.032 |    2.65%\n",
      "ViT_ps4_ed512_d8_h2_mr4                  |   25,330,276 |   18,472,036 |      0.039 |    2.33%\n",
      "ViT_ps4_ed512_d8_h4_mr2                  |   16,933,476 |   10,075,236 |      0.032 |    2.44%\n",
      "ViT_ps4_ed512_d8_h4_mr4                  |   25,330,276 |   18,472,036 |      0.039 |    3.04%\n",
      "ViT_ps8_ed256_d4_h2_mr2                  |    2,188,644 |    1,872,484 |      0.020 |    8.10%\n",
      "ViT_ps8_ed256_d4_h2_mr4                  |    3,239,268 |    2,923,108 |      0.021 |    6.81%\n",
      "ViT_ps8_ed256_d4_h4_mr2                  |    2,188,644 |    1,872,484 |      0.020 |   17.39%\n",
      "ViT_ps8_ed256_d4_h4_mr4                  |    3,239,268 |    2,923,108 |      0.019 |    9.71%\n",
      "ViT_ps8_ed256_d8_h2_mr2                  |    4,297,060 |    2,928,228 |      0.025 |    4.92%\n",
      "ViT_ps8_ed256_d8_h2_mr4                  |    6,398,308 |    5,029,476 |      0.026 |    4.91%\n",
      "ViT_ps8_ed256_d8_h4_mr2                  |    4,297,060 |    2,928,228 |      0.026 |    7.19%\n",
      "ViT_ps8_ed256_d8_h4_mr4                  |    6,398,308 |    5,029,476 |      0.026 |    7.69%\n",
      "ViT_ps8_ed512_d4_h2_mr2                  |    8,571,492 |    5,842,020 |      0.020 |    5.12%\n",
      "ViT_ps8_ed512_d4_h2_mr4                  |   12,769,892 |   10,040,420 |      0.020 |    5.54%\n",
      "ViT_ps8_ed512_d4_h4_mr2                  |    8,571,492 |    5,842,020 |      0.019 |    5.06%\n",
      "ViT_ps8_ed512_d4_h4_mr4                  |   12,769,892 |   10,040,420 |      0.019 |    5.23%\n",
      "ViT_ps8_ed512_d8_h2_mr2                  |   16,982,628 |   10,050,660 |      0.025 |    3.42%\n",
      "ViT_ps8_ed512_d8_h2_mr4                  |   25,379,428 |   18,447,460 |      0.026 |    3.36%\n",
      "ViT_ps8_ed512_d8_h4_mr2                  |   16,982,628 |   10,050,660 |      0.026 |    3.97%\n",
      "ViT_ps8_ed512_d8_h4_mr4                  |   25,379,428 |   18,447,460 |      0.026 |    4.20%\n",
      "ResNet-18                                |   11,220,132 |  555,478,500 |      0.021 |   61.41%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Vision Transformer Modules\n",
    "# -------------------------------\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, \n",
    "                              kernel_size=patch_size, \n",
    "                              stride=patch_size)\n",
    "        # learnable CLS token + positional encoding\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.n_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x)                  # (B, E, H/ps, W/ps)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, E)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B,1,E)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)          # (B,1+n_patches,E)\n",
    "        x = x + self.pos_embed\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp   = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, E) -> transpose for MHAttention: (N, B, E)\n",
    "        x2 = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x2.transpose(0,1), x2.transpose(0,1), x2.transpose(0,1))\n",
    "        x = x + attn_out.transpose(0,1)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 img_size=32, patch_size=4, in_chans=3, \n",
    "                 num_classes=100, embed_dim=256, depth=4, \n",
    "                 num_heads=4, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        mlp_hidden = embed_dim * mlp_ratio\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_hidden, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)   # (B, 1+n, E)\n",
    "        for blk in self.encoder:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]\n",
    "        return self.head(cls_token_final)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Data Loaders\n",
    "# -------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071,0.4867,0.4408), (0.2675,0.2565,0.2761))\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071,0.4867,0.4408), (0.2675,0.2565,0.2761))\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_ds  = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Training & Evaluation\n",
    "# -------------------------------\n",
    "def train_one_epoch(model, optimizer, criterion, loader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start = time.time()\n",
    "    for imgs, labels in tqdm(loader, leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    elapsed = time.time() - start\n",
    "    return running_loss / len(loader.dataset), elapsed / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        out = model(imgs)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Experiment Loop\n",
    "# -------------------------------\n",
    "vit_configs = []\n",
    "for ps in [4,8]:\n",
    "  for ed in [256,512]:\n",
    "    for dp in [4,8]:\n",
    "      for nh in [2,4]:\n",
    "        for mr in [2,4]:\n",
    "          vit_configs.append({\n",
    "            'patch_size': ps,\n",
    "            'embed_dim': ed,\n",
    "            'depth': dp,\n",
    "            'num_heads': nh,\n",
    "            'mlp_ratio': mr\n",
    "          })\n",
    "\n",
    "results = []\n",
    "\n",
    "# ViT runs (20 epochs each)\n",
    "for cfg in vit_configs:\n",
    "    name = f\"ViT_ps{cfg['patch_size']}_ed{cfg['embed_dim']}_d{cfg['depth']}_h{cfg['num_heads']}_mr{cfg['mlp_ratio']}\"\n",
    "    print(f\"\\n===== TRAINING {name} =====\")\n",
    "    model = ViT(patch_size=cfg['patch_size'],\n",
    "                embed_dim=cfg['embed_dim'],\n",
    "                depth=cfg['depth'],\n",
    "                num_heads=cfg['num_heads'],\n",
    "                mlp_ratio=cfg['mlp_ratio']\n",
    "               ).to(device)\n",
    "    opt   = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    crit  = nn.CrossEntropyLoss()\n",
    "\n",
    "    # summary for params & FLOPs\n",
    "    sumry = summary(model, input_size=(1,3,32,32), verbose=0)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    flops    = sumry.total_mult_adds  # approx FLOPs\n",
    "\n",
    "    # train\n",
    "    total_time = 0.\n",
    "    for epoch in range(20):\n",
    "        loss, tpe = train_one_epoch(model, opt, crit, train_loader)\n",
    "        total_time += tpe\n",
    "    avg_time = total_time / 20\n",
    "    acc = test(model, test_loader)\n",
    "\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'params': n_params,\n",
    "        'flops': flops,\n",
    "        'time/epoch(s)': avg_time,\n",
    "        'test_acc': acc\n",
    "    })\n",
    "\n",
    "# ResNet-18 baseline (10 epochs)\n",
    "print(\"\\n===== TRAINING ResNet-18 CIFAR100 =====\")\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "# adapt for CIFAR:\n",
    "resnet.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "resnet.maxpool = nn.Identity()\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "opt = optim.Adam(resnet.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "sumry = summary(resnet, input_size=(1,3,32,32), verbose=0)\n",
    "n_params = sum(p.numel() for p in resnet.parameters())\n",
    "flops    = sumry.total_mult_adds\n",
    "\n",
    "total_time = 0.\n",
    "for epoch in range(10):\n",
    "    loss, tpe = train_one_epoch(resnet, opt, crit, train_loader)\n",
    "    total_time += tpe\n",
    "avg_time = total_time / 10\n",
    "acc = test(resnet, test_loader)\n",
    "\n",
    "results.append({\n",
    "    'model': 'ResNet-18',\n",
    "    'params': n_params,\n",
    "    'flops': flops,\n",
    "    'time/epoch(s)': avg_time,\n",
    "    'test_acc': acc\n",
    "})\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Summary\n",
    "# -------------------------------\n",
    "print(\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"{'Model':40} | {'Params':12} | {'FLOPs':12} | {'Time/Ep(s)':10} | {'Test Acc':8}\")\n",
    "print(\"-\"*90)\n",
    "for r in results:\n",
    "    print(f\"{r['model']:40} | {r['params']:12,d} | {int(r['flops']):12,d} | {r['time/epoch(s)']:10.3f} | {r['test_acc']*100:7.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5b7f8-8a61-453e-acb1-102c6c114b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

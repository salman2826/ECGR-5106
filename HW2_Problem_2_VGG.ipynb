{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e8119c-d35c-406a-aca1-0200bdfa3b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 1.8259, Val Loss: 1.4919, Val Acc: 0.4237\n",
      "Epoch 2/20, Train Loss: 1.4540, Val Loss: 1.3050, Val Acc: 0.5129\n",
      "Epoch 3/20, Train Loss: 1.2453, Val Loss: 1.1294, Val Acc: 0.5919\n",
      "Epoch 4/20, Train Loss: 1.1003, Val Loss: 1.0126, Val Acc: 0.6324\n",
      "Epoch 5/20, Train Loss: 1.0057, Val Loss: 0.9109, Val Acc: 0.6772\n",
      "Epoch 6/20, Train Loss: 0.9187, Val Loss: 0.8651, Val Acc: 0.7000\n",
      "Epoch 7/20, Train Loss: 0.8637, Val Loss: 0.8479, Val Acc: 0.7136\n",
      "Epoch 8/20, Train Loss: 0.8149, Val Loss: 0.7790, Val Acc: 0.7358\n",
      "Epoch 9/20, Train Loss: 0.7622, Val Loss: 0.7286, Val Acc: 0.7530\n",
      "Epoch 10/20, Train Loss: 0.7370, Val Loss: 0.7039, Val Acc: 0.7567\n",
      "Epoch 11/20, Train Loss: 0.7129, Val Loss: 0.6947, Val Acc: 0.7655\n",
      "Epoch 12/20, Train Loss: 0.6841, Val Loss: 0.6673, Val Acc: 0.7766\n",
      "Epoch 13/20, Train Loss: 0.6625, Val Loss: 0.6696, Val Acc: 0.7726\n",
      "Epoch 14/20, Train Loss: 0.6443, Val Loss: 0.6345, Val Acc: 0.7848\n",
      "Epoch 15/20, Train Loss: 0.6306, Val Loss: 0.6506, Val Acc: 0.7799\n",
      "Epoch 16/20, Train Loss: 0.6140, Val Loss: 0.6300, Val Acc: 0.7848\n",
      "Epoch 17/20, Train Loss: 0.6041, Val Loss: 0.6326, Val Acc: 0.7891\n",
      "Epoch 18/20, Train Loss: 0.5859, Val Loss: 0.6237, Val Acc: 0.7932\n",
      "Epoch 19/20, Train Loss: 0.5839, Val Loss: 0.6038, Val Acc: 0.7934\n",
      "Epoch 20/20, Train Loss: 0.5732, Val Loss: 0.6028, Val Acc: 0.7935\n",
      "Number of parameters in the simplified VGGNet: 29715850\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simplified VGGNet\n",
    "class SimplifiedVGGNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_prob=0.5):\n",
    "        super(SimplifiedVGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Output: 64x32x32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x16x16\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Output: 128x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 128x8x8\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x4x4\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 512x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(512 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedVGGNet(num_classes=10, dropout_prob=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified VGGNet: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330b45e0-8fb6-4e90-b610-9d8b2a15a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 1.8644, Val Loss: 1.5231, Val Acc: 0.4147\n",
      "Epoch 2/20, Train Loss: 1.4135, Val Loss: 1.2670, Val Acc: 0.5373\n",
      "Epoch 3/20, Train Loss: 1.1801, Val Loss: 1.0491, Val Acc: 0.6251\n",
      "Epoch 4/20, Train Loss: 1.0041, Val Loss: 1.0210, Val Acc: 0.6394\n",
      "Epoch 5/20, Train Loss: 0.8961, Val Loss: 0.8483, Val Acc: 0.7110\n",
      "Epoch 6/20, Train Loss: 0.8144, Val Loss: 0.7976, Val Acc: 0.7256\n",
      "Epoch 7/20, Train Loss: 0.7408, Val Loss: 0.7407, Val Acc: 0.7476\n",
      "Epoch 8/20, Train Loss: 0.6985, Val Loss: 0.6725, Val Acc: 0.7697\n",
      "Epoch 9/20, Train Loss: 0.6588, Val Loss: 0.7271, Val Acc: 0.7601\n",
      "Epoch 10/20, Train Loss: 0.6241, Val Loss: 0.7123, Val Acc: 0.7631\n",
      "Epoch 11/20, Train Loss: 0.6040, Val Loss: 0.6014, Val Acc: 0.7953\n",
      "Epoch 12/20, Train Loss: 0.5766, Val Loss: 0.6341, Val Acc: 0.7879\n",
      "Epoch 13/20, Train Loss: 0.5600, Val Loss: 0.6503, Val Acc: 0.7756\n",
      "Epoch 14/20, Train Loss: 0.5443, Val Loss: 0.5938, Val Acc: 0.7990\n",
      "Epoch 15/20, Train Loss: 0.5242, Val Loss: 0.5783, Val Acc: 0.8085\n",
      "Epoch 16/20, Train Loss: 0.5111, Val Loss: 0.5615, Val Acc: 0.8178\n",
      "Epoch 17/20, Train Loss: 0.4957, Val Loss: 0.5792, Val Acc: 0.8084\n",
      "Epoch 18/20, Train Loss: 0.4806, Val Loss: 0.5563, Val Acc: 0.8131\n",
      "Epoch 19/20, Train Loss: 0.4708, Val Loss: 0.5471, Val Acc: 0.8170\n",
      "Epoch 20/20, Train Loss: 0.4576, Val Loss: 0.5565, Val Acc: 0.8189\n",
      "Number of parameters in the simplified VGGNet: 29715850\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simplified VGGNet\n",
    "class SimplifiedVGGNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_prob=0):\n",
    "        super(SimplifiedVGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Output: 64x32x32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x16x16\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Output: 128x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 128x8x8\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x4x4\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 512x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(512 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedVGGNet(num_classes=10, dropout_prob=0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified VGGNet: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365ca586-5b8b-44b8-aa7e-ce7119f072f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 4.3372, Val Loss: 4.1800, Val Acc: 0.0509\n",
      "Epoch 2/20, Train Loss: 3.8993, Val Loss: 3.7572, Val Acc: 0.1048\n",
      "Epoch 3/20, Train Loss: 3.6536, Val Loss: 3.5621, Val Acc: 0.1385\n",
      "Epoch 4/20, Train Loss: 3.4570, Val Loss: 3.3107, Val Acc: 0.1864\n",
      "Epoch 5/20, Train Loss: 3.2695, Val Loss: 3.1809, Val Acc: 0.2130\n",
      "Epoch 6/20, Train Loss: 3.0972, Val Loss: 3.0630, Val Acc: 0.2408\n",
      "Epoch 7/20, Train Loss: 2.9506, Val Loss: 2.8835, Val Acc: 0.2790\n",
      "Epoch 8/20, Train Loss: 2.8310, Val Loss: 2.7588, Val Acc: 0.3053\n",
      "Epoch 9/20, Train Loss: 2.6965, Val Loss: 2.6650, Val Acc: 0.3195\n",
      "Epoch 10/20, Train Loss: 2.5888, Val Loss: 2.6133, Val Acc: 0.3363\n",
      "Epoch 11/20, Train Loss: 2.4982, Val Loss: 2.5416, Val Acc: 0.3437\n",
      "Epoch 12/20, Train Loss: 2.4154, Val Loss: 2.4730, Val Acc: 0.3613\n",
      "Epoch 13/20, Train Loss: 2.3414, Val Loss: 2.4366, Val Acc: 0.3712\n",
      "Epoch 14/20, Train Loss: 2.2662, Val Loss: 2.3827, Val Acc: 0.3826\n",
      "Epoch 15/20, Train Loss: 2.1998, Val Loss: 2.3713, Val Acc: 0.3844\n",
      "Epoch 16/20, Train Loss: 2.1407, Val Loss: 2.3553, Val Acc: 0.3898\n",
      "Epoch 17/20, Train Loss: 2.0784, Val Loss: 2.3160, Val Acc: 0.4007\n",
      "Epoch 18/20, Train Loss: 2.0137, Val Loss: 2.3173, Val Acc: 0.4035\n",
      "Epoch 19/20, Train Loss: 1.9788, Val Loss: 2.2698, Val Acc: 0.4157\n",
      "Epoch 20/20, Train Loss: 1.9092, Val Loss: 2.2831, Val Acc: 0.4196\n",
      "Number of parameters in the simplified VGGNet: 30084580\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simplified VGGNet\n",
    "class SimplifiedVGGNet(nn.Module):\n",
    "    def __init__(self, num_classes=100, dropout_prob=0):\n",
    "        super(SimplifiedVGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Output: 64x32x32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x16x16\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Output: 128x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 128x8x8\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x4x4\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 512x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(512 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedVGGNet(num_classes=100, dropout_prob=0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified VGGNet: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a31b279-5aeb-4c03-9594-9ab67244d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 4.4051, Val Loss: 4.0772, Val Acc: 0.0482\n",
      "Epoch 2/20, Train Loss: 4.0120, Val Loss: 3.9226, Val Acc: 0.0776\n",
      "Epoch 3/20, Train Loss: 3.8382, Val Loss: 3.6990, Val Acc: 0.1112\n",
      "Epoch 4/20, Train Loss: 3.6836, Val Loss: 3.5364, Val Acc: 0.1441\n",
      "Epoch 5/20, Train Loss: 3.5373, Val Loss: 3.3510, Val Acc: 0.1746\n",
      "Epoch 6/20, Train Loss: 3.4167, Val Loss: 3.2432, Val Acc: 0.1893\n",
      "Epoch 7/20, Train Loss: 3.3180, Val Loss: 3.1654, Val Acc: 0.2190\n",
      "Epoch 8/20, Train Loss: 3.2288, Val Loss: 3.0538, Val Acc: 0.2407\n",
      "Epoch 9/20, Train Loss: 3.1515, Val Loss: 2.9780, Val Acc: 0.2539\n",
      "Epoch 10/20, Train Loss: 3.0754, Val Loss: 2.9118, Val Acc: 0.2663\n",
      "Epoch 11/20, Train Loss: 3.0057, Val Loss: 2.8379, Val Acc: 0.2766\n",
      "Epoch 12/20, Train Loss: 2.9512, Val Loss: 2.7504, Val Acc: 0.2942\n",
      "Epoch 13/20, Train Loss: 2.8926, Val Loss: 2.7475, Val Acc: 0.2974\n",
      "Epoch 14/20, Train Loss: 2.8503, Val Loss: 2.7468, Val Acc: 0.3007\n",
      "Epoch 15/20, Train Loss: 2.7937, Val Loss: 2.6637, Val Acc: 0.3147\n",
      "Epoch 16/20, Train Loss: 2.7554, Val Loss: 2.5948, Val Acc: 0.3336\n",
      "Epoch 17/20, Train Loss: 2.7169, Val Loss: 2.6451, Val Acc: 0.3239\n",
      "Epoch 18/20, Train Loss: 2.6806, Val Loss: 2.5646, Val Acc: 0.3342\n",
      "Epoch 19/20, Train Loss: 2.6481, Val Loss: 2.4946, Val Acc: 0.3517\n",
      "Epoch 20/20, Train Loss: 2.6113, Val Loss: 2.4998, Val Acc: 0.3557\n",
      "Number of parameters in the simplified VGGNet: 30084580\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simplified VGGNet\n",
    "class SimplifiedVGGNet(nn.Module):\n",
    "    def __init__(self, num_classes=100, dropout_prob=0.5):\n",
    "        super(SimplifiedVGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Output: 64x32x32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x16x16\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Output: 128x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 128x8x8\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x4x4\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  # Output: 512x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 512x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(512 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedVGGNet(num_classes=100, dropout_prob=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified VGGNet: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ebfc6-e998-43c7-a3ed-55a4c7615a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

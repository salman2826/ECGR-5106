{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e4c01e-49f3-496f-90a9-3fcdb8307a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e679b8-51b5-4317-8182-b32e1d5db812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 1.6742, Val Loss: 1.3472, Val Acc: 0.5031\n",
      "Epoch 2/20, Train Loss: 1.2274, Val Loss: 1.1424, Val Acc: 0.5939\n",
      "Epoch 3/20, Train Loss: 1.0246, Val Loss: 1.0440, Val Acc: 0.6286\n",
      "Epoch 4/20, Train Loss: 0.8921, Val Loss: 0.9598, Val Acc: 0.6653\n",
      "Epoch 5/20, Train Loss: 0.7655, Val Loss: 0.9447, Val Acc: 0.6872\n",
      "Epoch 6/20, Train Loss: 0.6612, Val Loss: 0.9155, Val Acc: 0.6939\n",
      "Epoch 7/20, Train Loss: 0.5648, Val Loss: 0.9116, Val Acc: 0.7094\n",
      "Epoch 8/20, Train Loss: 0.4844, Val Loss: 0.9343, Val Acc: 0.7172\n",
      "Epoch 9/20, Train Loss: 0.3977, Val Loss: 0.9880, Val Acc: 0.7052\n",
      "Epoch 10/20, Train Loss: 0.3367, Val Loss: 1.0480, Val Acc: 0.7101\n",
      "Epoch 11/20, Train Loss: 0.2903, Val Loss: 1.2598, Val Acc: 0.6914\n",
      "Epoch 12/20, Train Loss: 0.2354, Val Loss: 1.2129, Val Acc: 0.7118\n",
      "Epoch 13/20, Train Loss: 0.2043, Val Loss: 1.2040, Val Acc: 0.7075\n",
      "Epoch 14/20, Train Loss: 0.1765, Val Loss: 1.3387, Val Acc: 0.7024\n",
      "Epoch 15/20, Train Loss: 0.1546, Val Loss: 1.4277, Val Acc: 0.7052\n",
      "Epoch 16/20, Train Loss: 0.1393, Val Loss: 1.3811, Val Acc: 0.7108\n",
      "Epoch 17/20, Train Loss: 0.1172, Val Loss: 1.4836, Val Acc: 0.7158\n",
      "Epoch 18/20, Train Loss: 0.1073, Val Loss: 1.4800, Val Acc: 0.7035\n",
      "Epoch 19/20, Train Loss: 0.1039, Val Loss: 1.5912, Val Acc: 0.7086\n",
      "Epoch 20/20, Train Loss: 0.0999, Val Loss: 1.6571, Val Acc: 0.7158\n",
      "Number of parameters in the simplified AlexNet: 23272266\n",
      "Number of parameters in the original AlexNet: ~60 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define a simplified AlexNet\n",
    "class SimplifiedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_prob=0):\n",
    "        super(SimplifiedAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # Output: 64x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x8x8\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),  # Output: 192x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 192x4x4\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # Output: 384x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedAlexNet(dropout_prob=0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified AlexNet: {count_parameters(model)}\")\n",
    "\n",
    "# Compare with the original AlexNet (approx. 60 million parameters)\n",
    "print(\"Number of parameters in the original AlexNet: ~60 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "801e2317-cbb3-4189-8bb6-1cf4a8a940f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 1.6734, Val Loss: 1.3377, Val Acc: 0.5076\n",
      "Epoch 2/20, Train Loss: 1.2570, Val Loss: 1.1095, Val Acc: 0.6009\n",
      "Epoch 3/20, Train Loss: 1.0750, Val Loss: 1.0124, Val Acc: 0.6439\n",
      "Epoch 4/20, Train Loss: 0.9493, Val Loss: 0.9938, Val Acc: 0.6527\n",
      "Epoch 5/20, Train Loss: 0.8212, Val Loss: 0.9345, Val Acc: 0.6828\n",
      "Epoch 6/20, Train Loss: 0.7326, Val Loss: 0.9199, Val Acc: 0.6842\n",
      "Epoch 7/20, Train Loss: 0.6514, Val Loss: 0.8967, Val Acc: 0.7056\n",
      "Epoch 8/20, Train Loss: 0.5760, Val Loss: 0.9122, Val Acc: 0.7077\n",
      "Epoch 9/20, Train Loss: 0.5092, Val Loss: 0.8924, Val Acc: 0.7227\n",
      "Epoch 10/20, Train Loss: 0.4406, Val Loss: 1.0018, Val Acc: 0.7050\n",
      "Epoch 11/20, Train Loss: 0.3950, Val Loss: 1.0031, Val Acc: 0.7112\n",
      "Epoch 12/20, Train Loss: 0.3519, Val Loss: 1.0602, Val Acc: 0.7165\n",
      "Epoch 13/20, Train Loss: 0.3135, Val Loss: 1.1135, Val Acc: 0.7197\n",
      "Epoch 14/20, Train Loss: 0.2766, Val Loss: 1.1037, Val Acc: 0.7148\n",
      "Epoch 15/20, Train Loss: 0.2455, Val Loss: 1.2241, Val Acc: 0.7137\n",
      "Epoch 16/20, Train Loss: 0.2220, Val Loss: 1.2253, Val Acc: 0.7140\n",
      "Epoch 17/20, Train Loss: 0.2120, Val Loss: 1.2384, Val Acc: 0.7145\n",
      "Epoch 18/20, Train Loss: 0.1885, Val Loss: 1.3150, Val Acc: 0.7107\n",
      "Epoch 19/20, Train Loss: 0.1770, Val Loss: 1.3965, Val Acc: 0.7151\n",
      "Epoch 20/20, Train Loss: 0.1575, Val Loss: 1.3792, Val Acc: 0.7063\n",
      "Number of parameters in the simplified AlexNet: 23272266\n",
      "Number of parameters in the original AlexNet: ~60 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define a simplified AlexNet\n",
    "class SimplifiedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_prob=0.5):\n",
    "        super(SimplifiedAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # Output: 64x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x8x8\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),  # Output: 192x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 192x4x4\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # Output: 384x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedAlexNet(dropout_prob=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified AlexNet: {count_parameters(model)}\")\n",
    "\n",
    "# Compare with the original AlexNet (approx. 60 million parameters)\n",
    "print(\"Number of parameters in the original AlexNet: ~60 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa8ad42-81ef-47b7-b8b1-86cce26c9f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 169001437/169001437 [00:02<00:00, 75369816.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 4.1415, Val Loss: 3.8418, Val Acc: 0.0993\n",
      "Epoch 2/20, Train Loss: 3.6573, Val Loss: 3.4834, Val Acc: 0.1563\n",
      "Epoch 3/20, Train Loss: 3.2712, Val Loss: 3.1389, Val Acc: 0.2194\n",
      "Epoch 4/20, Train Loss: 2.9324, Val Loss: 2.8924, Val Acc: 0.2751\n",
      "Epoch 5/20, Train Loss: 2.6572, Val Loss: 2.7564, Val Acc: 0.3025\n",
      "Epoch 6/20, Train Loss: 2.4252, Val Loss: 2.6240, Val Acc: 0.3273\n",
      "Epoch 7/20, Train Loss: 2.2095, Val Loss: 2.5485, Val Acc: 0.3475\n",
      "Epoch 8/20, Train Loss: 1.9996, Val Loss: 2.5082, Val Acc: 0.3664\n",
      "Epoch 9/20, Train Loss: 1.8043, Val Loss: 2.5307, Val Acc: 0.3724\n",
      "Epoch 10/20, Train Loss: 1.6019, Val Loss: 2.6603, Val Acc: 0.3672\n",
      "Epoch 11/20, Train Loss: 1.4115, Val Loss: 2.7537, Val Acc: 0.3745\n",
      "Epoch 12/20, Train Loss: 1.2173, Val Loss: 2.9618, Val Acc: 0.3746\n",
      "Epoch 13/20, Train Loss: 1.0312, Val Loss: 3.2201, Val Acc: 0.3682\n",
      "Epoch 14/20, Train Loss: 0.8687, Val Loss: 3.3784, Val Acc: 0.3672\n",
      "Epoch 15/20, Train Loss: 0.7419, Val Loss: 3.6024, Val Acc: 0.3608\n",
      "Epoch 16/20, Train Loss: 0.6238, Val Loss: 4.0214, Val Acc: 0.3593\n",
      "Epoch 17/20, Train Loss: 0.5115, Val Loss: 4.3758, Val Acc: 0.3647\n",
      "Epoch 18/20, Train Loss: 0.4767, Val Loss: 4.3824, Val Acc: 0.3596\n",
      "Epoch 19/20, Train Loss: 0.4160, Val Loss: 4.6781, Val Acc: 0.3568\n",
      "Epoch 20/20, Train Loss: 0.3585, Val Loss: 4.9483, Val Acc: 0.3576\n",
      "Number of parameters in the simplified AlexNet: 23640996\n",
      "Number of parameters in the original AlexNet: ~60 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define a simplified AlexNet\n",
    "class SimplifiedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=100, dropout_prob=0):\n",
    "        super(SimplifiedAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # Output: 64x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x8x8\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),  # Output: 192x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 192x4x4\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # Output: 384x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedAlexNet(num_classes=100, dropout_prob=0).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified AlexNet: {count_parameters(model)}\")\n",
    "\n",
    "# Compare with the original AlexNet (approx. 60 million parameters)\n",
    "print(\"Number of parameters in the original AlexNet: ~60 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfc2d84-d558-45c5-866e-e564e129a905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1/20, Train Loss: 4.3477, Val Loss: 3.9295, Val Acc: 0.0703\n",
      "Epoch 2/20, Train Loss: 3.7956, Val Loss: 3.6155, Val Acc: 0.1182\n",
      "Epoch 3/20, Train Loss: 3.4723, Val Loss: 3.3484, Val Acc: 0.1760\n",
      "Epoch 4/20, Train Loss: 3.2075, Val Loss: 3.1222, Val Acc: 0.2216\n",
      "Epoch 5/20, Train Loss: 2.9997, Val Loss: 2.9972, Val Acc: 0.2516\n",
      "Epoch 6/20, Train Loss: 2.8263, Val Loss: 2.8310, Val Acc: 0.2804\n",
      "Epoch 7/20, Train Loss: 2.6730, Val Loss: 2.7366, Val Acc: 0.3046\n",
      "Epoch 8/20, Train Loss: 2.5426, Val Loss: 2.7374, Val Acc: 0.3136\n",
      "Epoch 9/20, Train Loss: 2.4219, Val Loss: 2.6665, Val Acc: 0.3186\n",
      "Epoch 10/20, Train Loss: 2.3098, Val Loss: 2.6831, Val Acc: 0.3268\n",
      "Epoch 11/20, Train Loss: 2.2080, Val Loss: 2.6399, Val Acc: 0.3389\n",
      "Epoch 12/20, Train Loss: 2.1160, Val Loss: 2.6638, Val Acc: 0.3381\n",
      "Epoch 13/20, Train Loss: 2.0319, Val Loss: 2.6212, Val Acc: 0.3533\n",
      "Epoch 14/20, Train Loss: 1.9508, Val Loss: 2.6574, Val Acc: 0.3532\n",
      "Epoch 15/20, Train Loss: 1.8727, Val Loss: 2.6841, Val Acc: 0.3516\n",
      "Epoch 16/20, Train Loss: 1.7927, Val Loss: 2.6722, Val Acc: 0.3625\n",
      "Epoch 17/20, Train Loss: 1.7370, Val Loss: 2.6644, Val Acc: 0.3601\n",
      "Epoch 18/20, Train Loss: 1.6861, Val Loss: 2.7511, Val Acc: 0.3558\n",
      "Epoch 19/20, Train Loss: 1.6224, Val Loss: 2.7797, Val Acc: 0.3586\n",
      "Epoch 20/20, Train Loss: 1.5759, Val Loss: 2.7380, Val Acc: 0.3596\n",
      "Number of parameters in the simplified AlexNet: 23640996\n",
      "Number of parameters in the original AlexNet: ~60 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define a simplified AlexNet\n",
    "class SimplifiedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=100, dropout_prob=0.5):\n",
    "        super(SimplifiedAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # Output: 64x16x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x8x8\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),  # Output: 192x8x8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 192x4x4\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),  # Output: 384x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Output: 256x4x4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 256x2x2\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimplifiedAlexNet(num_classes=100, dropout_prob=0.5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return running_loss / len(test_loader), correct / total\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Print the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of parameters in the simplified AlexNet: {count_parameters(model)}\")\n",
    "\n",
    "# Compare with the original AlexNet (approx. 60 million parameters)\n",
    "print(\"Number of parameters in the original AlexNet: ~60 million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff0a9a-d806-4987-800d-95a8bcefd1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
